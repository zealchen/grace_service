[Music] So, as you come in, we have here a server set up with everything you'll need. If you want to follow along, you should have gotten a post-it note. If you don't, just raise your hand and my colleague Alex over here will come find you and we'll provide you with one. Uh, basically what you're going to do is you're just going to go, if you have a number 160 or below, you go to this link here, the QR code on top as well. Um, and if you have a number that's 2011 or above, you go to the second link or the QR code. From there, I'll give you some directions.
You're going to have to clone a repo.
Um, and then you're just going to have to move an environments file over really quick. Also, for everything in this deck, I did create a workshop graph intro slack channel. So, if you're part of the AIE uh Slack group, you can also go there and just grab this deck um and you know, get the links or however however you'd like to do that. So, we'll get started here, I think, in just a couple of minutes.
All right. So, your sorry your your number will give you your username and your password.
Basically, it's attendee all lowercase than your number. That'll be both your username and your password when you sign in.
The other link that you should try to open as well is the um browser preview, but I'll walk you through that here in a second. I'll give it just another minute here for everyone to to file in and get situated.
What is the username and password? The username and password is going to be attendee all lowercase and then the number that you have both the username and the password are the same.
Question if these are going to be live um after the session the notebooks. So the notebooks uh the servers will be down um after the session but you have the GitHub link you can go back to um so the code is is there for you to use um it's just that the environment won't be available afterward.
All righty. So I'm going to go ahead and get started here. I'll leave this screen for a second. So if you want to grab the QR codes here, now would be the time to do it. Um, obviously you can go to the Slack channel and also pick up this deck.
All righty.
So, we're going to do an intro to graph workshop today. Um, I was debating what to actually put in this workshop since everything's changing so quickly. Um, and some of my colleagues convinced me not to make it too complicated. So this course is going to be very much an introductory level course. Um if you want to look at sort of more advanced graph techniques, integrations with things like MCP, we have that at our booth. Um and we have some other things that we're doing other events um tomorrow that will go over some of that stuff and I'll have links to all of that uh as we go through. But basically what we're going to do is we're going to get everything set up. Hopefully that should only take a few more minutes. And then we have three modules. We're going to go over some graph basics. We'll be using Neo forj today. It's a graph database.
Just how to query that kind of how to, you know, construct um your logic to retrieve data. We'll go over um another module on unstructured data um and how to do entity extraction.
How many people know about Neo forj? All righty. How many people have by the way used Neo forj like have written cipher queries.
Okay. So some folks in here. And then how how many people have used lane chain before? Okay. So so a fair number of you. Okay.
That's good to know. Um and then in our in our third module we'll actually go over we'll use lane graph. We'll build a very simple agent that will use some retrieval tools and you'll get to see how some of that works and we'll wrap up after that with some resources. So, make sure to ask questions straight away.
Raise your hand. I'll stop intermittently. Um, we only have 80 minutes though, so I want to make sure if you do have a question um go ahead and raise it and we'll get that answered because we're going to be moving through the material um a little bit quickly.
Um, as I said before, we have two Jupiter servers set up. So, you don't need to pip install anything. Um, you can go ahead and connect to these notebooks. Attendees, I already explained, you should have a number. If you don't, go ahead and raise your hand.
Um, the username and password is just going to be attendee followed by your number. 160 and less, you go to that first link. 2011 and larger, you go to the second link. Um, there's also I if you can go ahead and open browser.ne.iopreview.
Um, I'll show you in a little bit.
You're going to log into that as well. that will let you visualize the graph a little bit better as we start putting data inside of it.
Yes.
All right. So, Alex, um, we can get a number over here.
Once you're inside of your environment, what I want you to do is these two commands. So, you're going to open up a terminal window in Jupiter. You can do that by pressing the little plus sign. I want you to get clone. The command should actually be in the read me. It should say get clone and it will give you the link to the repository that you need to clone. And once you do that, I want you to copy the workshop file over into that Genai Workshop talent folder.
That environment file WS.N is going to have information to a database that's already set up. It will also have an open AI key inside of it that we'll be able to use for the workshop.
So, just to show you what this looks like, um, if I go over here and I look at my my terminal, right, I've already done this, this a little bit bigger. You just go ahead and get clone.
Um, if you go to the readme that was um in the main folder, this readme here, it has the uh the um the GitHub URL. So, you just basically go get clone. Um, and then oops. And then after that, you just copy that workshop file into that GEI workshop talent directory. So, you'll get this GNAI workshop talent. It's like a subdirectory in here and you just have to copy that file and that will have um resources that you'll need to log into uh the browser and for you to connect through your notebooks. The other link um that is inside of that deck is this browser.ne.preview link. Um basically that will give you a way to visualize the graph. So what you will do if I go ahead and disconnect um and maybe I um go to connect to an instance. So you should get a screen that looks like this. And then that workshop file will have and it should actually be the same thing for you guys. It should be the attendee, the number and then the same thing for the password. So go ahead and make sure you do that because then you'll be able to visualize the graph a little bit better.
Any questions so far? So for the Jupiter environment, if you got your number, it's attendee all lowercase and then your number for both the username and the password.
Sorry, where's the connection? Um, yeah, ws.m for the for the Neo Forj browser.
So, it's um right here.
So, it's basically going to be your username or sorry, it's going to be the number that you receive for both the Yeah. And if if you um want to come back to this if you have if you're connected through Slack the workshop graph intro, you can go there and you can pick up the slides as we move on. So then that way you just have a constant reference back to it.
Um so while everyone gets set up here I'll talk a little bit about just what graph rag is in general to kind of motivate what we're doing here. So this is um an architecture actually um that represents what some of our customers do. It's a very common um architecture for graph rag users. It's generalized and basically the idea is that you have your agent over there. You have your AI models and your your UI. So like all the normal things that you might think of if you're putting together a knowledge assistant. Um but then there's this knowledge graph thing in the middle. And that knowledge graph thing you can ingest both unstructured and structured data into that. So unstructured being things like documents and PDFs and that sort of stuff. And then structured being tables like CSVs or stuff from a relational database or what have you.
Um, and so there's a big question of like, well, why in, you know, the heck do we need this like knowledge graph thing in the middle, right? Like we have agents, we can have tools, and we can go pick stuff from data sources. Um, and so the idea with this is that if you have a use case and you kind of know the types of questions that you want to answer with your Asians, by taking your data and decomposing even a very simple knowledge graph to start, um, you're going to be able to expose um, a lot of the sort of domain logic that you'd want to apply through the model of your data. Um, so the idea is like we'll see when we build a skills graph, we'll make some relationships about people knowing skills. And by making that schema available to the agent and making tools available to the agent, um, you're going to be able to have a lot more control over how data is retrieved more accurately, explain the retrieval logic better. Um, and we see this is especially important as we start moving more and more into this agentic world because it's not like a oneshot vector search anymore, right? We're starting to see that now when we get questions or prompts handed to um an agentic workflow, those start to get broken down in various ways. Um and when you have a knowledge graph, it just lets you um offer retrieval logic to complement that in in a in a much more uh simple and and in my opinion a better manner.
And today we're going to be looking at a skills and employee graph. Uh so basically what we'll the use case we'll be looking at is you're building a knowledge assistant to help with things like searching for talent, aligning and analyzing skills within an organization um and doing things like staffing and team formation and substitutions and things of that nature.
And so I'm going to present a little bit about what we're going to go through in these modules first. So I'll do some stuff inside of a deck. Hopefully it won't take too long. I just want to kind of talk to you about cipher and some of the things that you'll be seeing and then we'll go ahead and get hands-on here pretty quickly.
Um, so we're going to talk about creating a graph. We'll start with some structured data here just to keep things simple. I'll introduce unstructured data a little bit later. Some basic cipher queries, some algorithms, and we'll get into some vector search and semantic stuff.
Um so a knowledge graph basically um when we think about it a knowledge graph generally is divi is defined as some as design patterns to organize and access interrelated data and at neo forj we model the data inside of the database is what's called a property graph and this consists of three primary elements so the first are nodes these are like your nouns these are your people places and things next are your relationships these are how things are related together, hence the name. Um, and often will be like verbs. So, person knows person, person lives with person, person drives or owns a car. And both of the um nodes and relationships can have properties which are just attributes. They can be strings, they can be numbers, they can be arrays of things and they can be vectors as well. So, we can store vectors for we've had for a long time inside of Neo forj. um and you can do search over these things. Now the uh query language that we're going to use to access the database is called cipher and I know a lot of you raised your hands in the beginning so you already have some familiarity with this. Um but cipher kind of looks like asy text. So the idea right is that um it has this SQL S kind of feel to it but you get to write these statements like if you see match person knows skill um basically you're connecting a person node to a skills node through that nose relationship. So it reads kind of very um literally in the way that it's written. Um nodes have what's called labels uh which is sort of like it would be the equivalent of a type of table within a SQL database of basically what type of entity it is. Um and then as I said before they have property. So for example you can identify by a property like name. Um and you can have variables like P and S which refer to the actual um entity as you start to write your query more. So, this is not going to be a course on writing cipher, right? Because we can make an 80-minute course just on like how are we going to make cipher queries. Um, but we will be walking through these queries. So, don't expect to like be if you haven't seen Cipher before to be a super expert in the cipher query language when we're done. Um, but just know that this is kind of how it works. And then as you go through hopefully you'll get a better understanding and a feel for how these queries uh work and and the types of data that can be returned as you run them.
Um, and so I'm sure is everyone pretty familiar with vector search in here at this point? Yeah, I have a feeling this audience probably would be. So I won't spend too long on this, right? I think we all kind of know what embeddings are.
It's basically a type of data compression. You can apply them to all sorts of things, right? Text, audio, you can even apply them to graphs. Um, oftent times it's just going to be a vector of numbers and then you can use that to find similar things within that domain space. So find texts that are similar uh semantically not just lex lexally like actually based on um the types of things that they're talking about. Um and within Neo forj you have search indices including vector. So there's range indices you have uniqueness constraints you're able to search text you're able to do full text with lucine. Um and then we also have approximate nearest neighbor vector search as well that we'll be leveraging um as we go through in combination with the cipher queries that we were just looking at to do graph traversals.
Um the next thing to know about is that in addition to being able to query the database, we also have analytics. So we have graph analytics powered on the database that lets you do different types of data enrichment um and do more graph global type of analytics. So finding which nodes are most central according to different algorithms, doing things like community detection, how do you cluster the graph, finding paths between nodes, doing different types of embeddings. Um so we have a lot of those algorithms and we'll be touching on them very very briefly today in the first module just to show that you know once you have a knowledge graph you can start enriching that data and then actually using things like we'll see in our case we'll be using community detection um where we'll be summarizing skills inside of our graph and then we'll be able to pass that on to an agent to actually use that to explain um some parts of our graph uh for our use case.
All righty. So, with that in mind, we'll go ahead here and jump into the first notebook. Um, are there any questions before we dive in? Is anyone still Okay. Yes. Over here.
Um, yes. Do you have uh Let me just go back here to the So, and this is available in the Slack channel, too. If you don't have a number, um, my colleague Alex over there can go ahead and grab one for you. Um, we're in the workshop graph intro Slack channel, so you can go there to grab the deck and all the links. But basically, if your number is 160 or below, you go to that first Jupiter server. If it's 2011 um or above, you go to the second one. Um you use attendee all lowercase and then your number as both your username and your password. You'll do that for the Jupiter notebook and then also for the Neo Forj browser if you want to follow along with visualizing um the graph as we go through.
Any other Yes, I know this is introduction to um maybe know when you're building I see you smalliz.
So your question is about data modeling and whether how do you prioritize making one graph versus multiple graphs. Um I mean it's a good question. I think in general for a lot of what we're seeing with agents I find it's helpful to have a smaller data model if possible um especially if you're doing different types of dynamic query generation um so to keep that in mind but as things are getting better we can pull back the graph schema and and offer it to agents and I'm we're noticing that as agents sort of keep iter or as language models really keep iterating they're starting to get better and better at interpreting so whenever you want to do traversals in a low latency a between two data points.
Those things really should go in the same graph and then it's a question as far as what you make a label versus a property um in that scenario. So we'll go through some of it and then if you want to talk after and come by our booth we can have a more sort of use case focused conversation. Anything else? All righty. So I'm going to go ahead here and then dive into the notebook.
So for our first notebook can go ahead and restart. That's fine.
So you're just going to come down here and start. Um and remember we're in the uh talent subfolder. So there's two workshops in here. The one we'll be doing is called talent. Um, if you're in the other one, it's there's also some interesting stuff in there, but you won't be able to follow along.
Alrighty. So, it looks like I'm running now. So, basically what I'm going to do is I'm going to get my environments file here, and I'm just going to load it. If you um don't have the environments file, just go ahead and move it. It's in the root directory. Just go ahead and move it into this subdirectory. It's this WS.N file.
Um, and basically what we're going to do first is we're just going to load our skills data sets. It's going to be a table.
And if we look at that table, um we're going to have uh basically three fields.
There's the um an email field, a name field, and then just a list of skills for the person. And as I said before, we'll go into a little bit of detail here around how you might extract this from documents like resumeumés um in a second. Um but basically for now because we're interested in sort of this skills uh mapping and team formation and staffing kind of use case, we're starting with this sort of very simple data set to get us started.
Um, and so there's a couple steps here that just go through basically organizing the data to make it easy to load. And then we're going to start to create our graph. And so a lot of this is just what we'd call like basic kind of NeoRaj data loading. We're going to create chunks out of our data frame.
You're going to um basically check to make sure you've got nothing in your database. I do have stuff in my database because I was just running this before.
Um, but that's on me because I was just running the course before. Yours should say zero. Now, the first thing we do is set a constraint. So, basically inside of Neo forj, whenever you create nodes, um, if you have what's called a node key constraint or a uniqueness constraint, it's basically saying in this case that the email has to be unique and nonnull for all your um, for all your people. and that will make it so that it's very fast to match on people and do merging operations. So, a lot of times people will say, well, Neo Forj is really slow.
Um, and that's often because of simple mistakes like not setting a constraint and then you're going to have to do very complex searches in the database every time you search on a user rather than having it um in an index that's unique.
Um, and then you also do the same for skill because our data model is going to be person and skill. So, we have two types of nodes.
Um, and when we do that, we'll go ahead and have two constraints here inside of the database. You'll see for skill and for person. Um, after that, we'll go ahead and start loading our nodes um, and our relationships. So, the way that this query works, and I I guess I won't run it, even though it won't actually change anything in my database, but what we're doing here is we're looping through chunks of our data frame, and we're saying, hey, merge a person on email, set their name, and then for that list of skills, basically, you're going to merge a skill on a skill name, and then you're going to merge here that the person knows that skill. So, it's going to create this graph pattern of person nose skill in the database.
Once you run that, what you can do is if you have that browser window open that we were going over before is I can go ahead and copy one of these or maybe I'll just take I'll take this one. Well, I'll go ahead and take this one first.
What this will show you inside of the database is if I just match people. I'll get my people back here. And I may have lost Oh, cool. I still have my internet connection. All right. So, I can go ahead and see that I have my people.
They have their names and their email addresses.
Um, you can do the same thing for matching skills.
And then, um, you can also look for relationships. So this gets into that pattern matching that we were talking about before with cipher. This is a very simple version of matching a path. So I'm saying P which is path is equal to node connect to nodes connects to another node. And I'm saying limit 25.
And that will return a graph where I get to see all these different relationships.
Looks like my internet connection is still somewhat slow, but I get it back here. So you'll see I'll get my people.
I'll get that nose relationship. And then in this case, this person knows API design, Tableau, Flask. Um, and you'll see different skills pop up here inside of your graph.
Um, and there's, you know, you can you can go ahead and run these through what through our driver here as well to look at the data, um, pull back the different people that are in there, um, and find out what skills they have and such. We do here.
Yeah.
Nose is a relationship type. We are making it up. So our our domain model that we have um I can actually call it here and mine is going to show more than yours if you run the same command because it has the um some other later stuff that we do in the course. But basically you have person knows skill.
That's our data model. So you can say person has skill would be another way to put it right. Um the word is just something that exactly there's an edge there.
Exactly. Yeah. And it's it's actually funny because this is becoming even more important um now that we're using uh LLMs to design queries because like the language that you use is sort of like an annotation for the model, right? So that starts to become very interesting.
All righty. So there's some cipher queries here that I'll go ahead and run through really quick. And I may uh depending on time need to need to kind of speed things up through this notebook because I want to make sure that we actually get to the agent at the end. Um so um if you go into the uh the deck there's this link browser.ne.iopreview and then I think it's just your username and your password. Um but you can also look inside of your um workshop environment file and it will have that information there. You just use your username and your password and your URI information. Um what you get here. So you get your URI and then your username and your password.
All righty. So like I said we'll go through some of these. So for example we can count in cipher. So we can say match person no skill. we can get back the name and we can count the distinct people uh basically here for for each skill. So basically what we're doing here is you can think of it as like okay I've got all my skills and I'm going to count the distinct people that know that skill. It's very simply what we're doing. When we get that back we'll see kind of what our most popular skills are here. Um going down and they're all kind of tech focused.
We can also ask different types of multihop questions which is very interesting. So, for example, I'll take this and I'll copy it over to my browser because it's it's interesting to see these visually. Um, but what we're asking here is we're going to we're going to take this person um named Lucy and I'm just going to ask, you know, what people are kind of similar to Lucy in terms of knowing the same skills, right? So, I can go ahead and run that. And then what I'll get is I'll get Lucy here. I'll get all of her skills and then I'll get all the other people that know those skills here, right? Um, and you can build on that iteratively. So, I can if I go back here, I can also say, well, now I want to know all of those skills or all of those people and I want to know basically I'm going to add at the end of that query. Um, I get they know a certain skill and then I want to get all of those people and then I want to get all the skills they know. So I'm basically adding this and what skills do these other people know to the query and then I'll get a very large graph back. But the idea with this is that once we have this logic extracted from whatever our original data source is, we can now control at a much more fine-tuned level how we define what a similar person is or what a similar skill is because we have this ability to traverse over the graph um and apply um concrete logic.
It's basically like having your information in a symbolic versus just a sub symbolic vector. Um, and so, you know, you'll get a lot of stuff back because now we're looking at people and all the other skills that they know. And I can go in here and find the most central skills among these people, right? Like, for example, scrum is very central among this group. Um, because there's there's a lot of people that know that skill. So, I'm figuring out about this local community that sort of uh knows similar skills to Lucy.
Um and and in here it's just some examples of running that uh same logic um basically inside of the inside of the notebook. Yes. forj browser link.
I'm over there. I'm over there. I mean, oh, I see. Um you table then graph and then sometimes if you're not returning nodes, it will only return a table. Like if I said, you know, return um yeah. Okay.
Okay. Yeah. So if you if you just say return p um in that case it should return it should return the pass.
Sometimes if you don't see it it's because you're returning like just a name or something. Um and then in that case it'll like just show you a list of names.
Follow question. What is a distinct doing in this case? Um and I don't have a distinct here.
Oh yeah I did. Um I don't know if it's completely necessary for this one actually. Um yeah, I don't think it is completely necessary for this one. Um there are times when you do very complicated especially we'll see that there are a couple other examples where we do like multihop paths and there's a chance with some of those that you'll get basically two paths that are the same. um in which case having the distinct there just allows you to um filter it better.
Yep.
So that's hard to say. They're getting better. Um a lot of it depends on the complexity of your schema. Um and basically you know we see for simpler aggregation queries or when you have a lot of prompt engineering around doing different types of path queries that are very specific on a smaller model they can do well. Um we do often recommend that you have your own expert tools if there's like a really complicated type of traversal that you want to do. So right you can write your own Python functions or you can have your own MCP server that will just have like your you know set of functions for your more complicated traversals. Uh we also see too you can sort of restrict the options for LLM. So instead of writing a complete query you can say hey like there's you know these you know three types of general patterns you know and and write that part of the pattern and then it will go into this other query.
So you can do stuff like that to help it. Um we've also done we've had fine-tuned models that we just released um I think back in April. Um they're on they're fine-tuned from GMA. They're on hugging face. Um so you can try using those as well. they can do a little bit better.
We're not going to use them here though, unfortunately, because we're using a bunch of OpenAI uh OpenAI models for this.
Um, all righty. So, a lot of this, um, as as I was just going over is basically just running these queries. Um, returning in this case, the distinct is important to return a distinct name because you might actually get to the same person multiple times. Um, so if you're just returning a name and a an um of a skill, right, then it's important to um to to use distinct in that case.
So we get all the distinct skills that basically showed up in the graph we were just looking at. Um, another thing that might be important for our use case is finding similar people. Um, so this is again using that uh query that we were just going over to find. We used Lucy before, but now we can actually uh parameterize that here. Um and then basically go no skill and then we can match um basically from that skill going to another person and we can sort of count number of shared skills between people.
Um and so when we do that right um we can go ahead and see in this case um like the number of shared skills between um between different individuals.
Um and we do it again here I think for a different set of people. I think this just counts most skills shared between any two people. Um so you can kind of see that here. Again, just another way to measure similarity beyond just semantic similarity, measuring actually um what we have from our model in terms of exact shared skills.
Um and as we go through this, some of the things that we can do to help speed up our queries, and this is this is sort of optional, but if we know that we're going to um sort of look for similar skill sets a lot, we can create a similar skill relationship inside of our graph. Um so basically we can match two different people um and then we can merge uh a similar skill set basically be based on um an overlap of the um of a skill count. So we have our data frame locally that we can use for that that we that we were just looking we basically just pulled it back when we were looking at those similar skills. And what that's going to do is it's just going to create again this uh relationship that has similar skill set between people. Um, and if I were to look at that, it will, go over to my browser.
We'll go ahead and see, you know, I can have a similar skill.
Some of these are overlap one, others will be greater. Um, inside of here, I think they go up to three. Um so it's all this is doing is basically saying hey if two people like what is their overlap so we don't have to do that full traversal over and over again if we don't want to um with that similar skill set relationship.
Um the next thing I wanted to show you and yes but if you do this you need to like this is static and if you want to update what's the overlap you need to run this query again.
You would need to run that query over again. Yeah. So, I mean, it it depends on how often your data gets updated. There's nothing actually wrong with doing the multihop query over and over again. Um, the graph database is designed to handle that. So, if you had a situation where you had a graph that was um constantly getting updated, you you might not even need to create this relationship.
Um, the next thing that I wanted to show you um was how our graph analytics works um inside of Neo Forj. and basically using that to uh enrich the graph. So this is um basically creating what we call a GDS or a graph data science client. Uh and what's going what we're basically doing here is we're creating something called a projection and then we're running a algorithm called Leiden um for the graphite community. Are you all have how many people here have heard of Leiden as an algorithm? Okay, so just just a few people. How many people have heard of Louain as a graph algorithm? Okay, so we got a couple people.
Basically, what this algorithm does is it breaks the graph down into a hierarchy. So, it will start by um basically breaking the graph into a few big communities and then going into smaller communities. And what it's trying to do is optimize what we call modularity. And it's basically this metric that says hey I want to create these clusters in my graph where the connections within the cluster are very high and connections across clusters are very low. So I'm creating these modules and basically what I do by creating these um and I'm using that similar skill set relationship. So this is another important reason to create it because if you do um analytics on your graph um it can help with those analytics running a little bit better because I have person connects to another person with a similar skill set.
Um and by running this lid in algorithm basically what we get is uh a bunch of communities that reflect people within the communities knowing similar skills.
Um, so this is all simulated data, but basically if I go down to um, and I'll skip over some of this. We do some checks around like how good the communities are. I encourage you to run this just so that the agent works well at the end. Um, but the what I wanted to show you here is this uh, graphic right here. So what this is this graphic is looking at because basically we wrote this community ID property back to the graph and you get these different community ids and you get to see which communities in a heat map have the most skills in a certain in a certain area.
So this data is randomly generated. So a lot of these patterns are going to look maybe a little bit funky if you were to really dig into them. But the idea is that um as you have very if as you have more relevant data and realistic data this can actually show you like your data engineers are here right and your front-end guys and front-end folks are over here and then your ML people are over here so you can start to see that within the graph um and really break that down. Um but do go ahead and run everything here um through the g. So that you have that property. Uh another way that we can break down uh sort of different groups inside of the graph is to look at Go ahead. Sorry.
I just had one question regarding when do you customize your community that you're running? actually better to figuring out well I think it depends on your use case right like if you're very interested in you know saying hey I want to understand like the skill communities inside of my company right if that's like a question that's going to come up frequently then using something like graph analytics can be very beneficial, right? Because you can do basically like employee segmentation and you can understand performance with inside of different groups and stuff. We see it oftent times used for customer segmentation and recommendation systems and that sort of thing too. Um at the same time, if you're just like, hey, I just want to like look for matches of different people with similar skills, maybe you don't need community detection for that because that's just like a pairing exercise, right? Um, so I'd say you use it whenever you want to do some sort of clustering analysis and persist that and then sort of even have visibility and I guess the the confidence in knowing that there was some way that you did that, right? And it's not just up to the model that's just making stuff up around how that works, right? So basically you look at what the users are doing and then trying to see Yeah. Yeah. Yeah.
Secondary skills.
The heat map is showing you how often different skills show up with inside of each community.
Yeah. So like the first community for example is it looks like either Tableau or Swift right? Yeah. To understand like the skill breakdown within each community. And again this is generated data. So, this is a little bit random, right? Um, but you can imagine that in a non-random scenario, what you're probably going to end up seeing is like if you have a lot of product managers versus a lot of, you know, um, like front-end developers versus, you know, like DevOps folks, like you'll see that grouping start to emerge.
Two connected questions. One, do you have any different best practices for data modeling for an agent to understand the data model or general graph best practices around creating data models.
Um yeah, I mean I'd say a lot of the agent stuff is evolving super super quickly. Um you know as LLMs keep changing and getting better. Um we've had for a long time guides on how to do like data migration from relational systems to graph and how to think about that. There's a certain way in graph how you think about again like nodes being nouns, relationships being verbs and how to connect those together. For agents, I think it's really nice when the data model reflects a natural language, right? So, person knows skill. It's very natural language way of, you know, saying something that that translates directly to a data model. And as I was saying before, simpler data models seem to work better when you do like dynamic query generation. Um, so there's stuff like that. And the rest of it is I know like the it depends answer is like you know such a copout but it is true that like depending on the type of retrievers that you have the size of your data um the cardality of different categories of things in your data right like you know you generally don't want if you can avoid it to have hundreds or thousands of node labels because it's just a lot.
So then you make them properties. So there's a lot of stuff like that to consider. So I don't know if that answers your question. Um the first part is how you have seen it be effective to show the reason what sort of schema file would you give it or what yeah we'll see at the end of module three which we might not have time to get to but you'll see it in the code and I'll I'll show it really quick is that there's you can from the graph schema there's um functions that we have to pull back the node labels and the relationship ship types. So you can create a sort of JSON representation, right, of what the graph schema looks like um and then combine it with specific prompts. So then it's like okay, I follow that. Another um thing to do that helps even more is if you have a graph data model that's not going to change a lot over time where you know you can just pull it and it will be the same for a while is you can annotate that schema. So you can say like for specific properties or node labels or relationship types hey this thing does this and when you ingest data make sure you you know put it here and when you pull data make sure you can you know go on different paths. The other thing is putting in like we had person nose skill. Putting those actual query patterns into the schema as well helps a lot because the model can read that and then understand how to do that traversal All right. Anything else? Right. Cool. Um so we're actually getting close on time. So I'm going to go pretty quickly through the rest of this. Um but hopefully um it'll be pretty understandable. So there's another way that we can start um thinking about skills and relationships between skills is how they're semantically similar. Um so basically what we can do is actually make embeddings on our skills. So there's another file in here um that basically has a CSV file that you read into this notebook that has skills and descriptions and an embedding. So which which field here do you think we embedded the skills or the description and why right so one of the things is when you have really short names like R is a technically a programming language although a lot of people don't love it I love R um AWS like they're very short right so having descriptions about those uh skill names if you embed those it provides a more informative embedding heading, right? So, that's the whole idea there. So, basically, we give each skill a description and then we embed that description. And what we're seeing inside of this is we're actually going to um and these are all text embedding ADA, so they're so they're 1536. Um we're going to go ahead and create a vector property. Um this is just loading those up in chunks. Um and then we're going to set the description as well.
And after we do that, um, we'll basically have I think we create our vector index down here that we call the skills embedding.
Once that's set up, basically what we're able to do, and you'll see it show up here. We'll get that skills embedding index, is we're now going to be able to do vector search on skills inside of the graph. So, if I have Python as a skill and I go ahead and I'll use this command in Cipher to search the skills embedding, pull back the 10 most relevant skills. Um, and you'll see here it'll it'll bring some skills back. Um, here it's like Ruby and Java. We got pandas at least that's good. Django, PyTorch. Um, so some of these are better than others, but the point is that we can go ahead and apply these vectors um and then pull information back with vector search.
And another interesting thing that we can do um as well is we can if I had something that wasn't in the database like say I'm just looking for API coding right and I searched that as a term basically what I'm doing here is I'm using the open AI uh client here to just embed this model or I might be using actually lang chain up here looks like it's lang chain um and then I'm doing a search on the database to pull back relevant skills with a certain similarity threshold and I'll get back API design and JavaScript for that API coding example.
Um, and what I can actually do in this case is I can say well I have this ability to do semantic similarity in the database. I can actually write a relationship that's just similar semantic and I can attach a score to that. Um, so there's some advantages to doing this, but a big one is visualization and also clustering. So if I were to take this command, which takes a semant similar semantic relationship and I go into my graph and I just put that in here and I just return all basically the skills that are semantically similar.
This internet speeds up hopefully. and I zoom in, I'll start to see sort of interesting groupings here. So, I'll start to see for example that I get my cloud skills here, Azure, AWS, cloud architecture all in one place.
Similarly, like I have Flask and Django here connected. I've got my data analytics group. This is like Tableau, PowerBI, data visualization. Um, and then I've got a big grouping over here.
So you see like you have your JVM languages like your Java and Scola and Cotlin here. Um and then I've got you know my Python stuff here with pandas.
And then if I go up in this group that's connected right I've got my Java and then I've got like all this front-end you know frameworks and stuff up here.
Um so don't underestimate the power of being able to visualize similarities.
Very important because I can create communities from these. I can use this for customized scoring in my retrieval queries which we'll see. Um, but the other really cool thing is that um, if for some reason like maybe I don't think Java should be connected to Python, I can control that. I can remove that relationship and then every time I do similarity relationships, I have control over that and I can filter that, right? Um, so that's just some important things to keep in mind about how you can um, sort of use vectors and graphs together.
Quick question. Is it only when you do semantic similarity? Is it only to visualize or any other? We'll see in a bit here because basically what what we can do and I'll answer this actually as I as I go down. Um I can pull back this semantic similarity relationship here.
But what I can start to do um which is actually pretty cool is I can start creating these um sort of customized scorings between things that kind of balance the like the semantic similarity versus hard rel like skill matches and I can weight that if I wanted to in in a custom way. So you can use it in your retrieval patterns as well to improve things. pattern would you go directly or would you vector search again? Do you use both typically or you so you can use both. Um so there's a lot of workflows because now you can compose things together into multi multiple steps, right? So you can definitely do something where you can pull similar skills and then look for people, right? And it just depends on how you break down those functions for the agents.
Sometimes if you know that like there's a very specific pattern that you want to follow. Like here, this is a very this looks like a really big query. It's somewhat intimidating, but it's actually not that complicated. Like what you're doing here is you're just sort of doing a waiting between the semantic similarity and a hard like overlap with similar skill sets. Like that might be a case where it actually coupling that logic together might make sense. Like if there's a very specific type of metric that you want for similarity.
What if you're coming from when you're coming from you have a much largerology like tell me about people with certain skills. How do you know that these are looking at what's the first step when you go from to retrieval? How do you break it into the entities to know that this is the second or this is the you should be doing? Does that make sense? Yeah. I mean, why don't we revisit that when we get to the third module? The second module should be really quick and then we can you can see some of the functions in the third module and then that might help me answer that question a little bit better. I think I know where you're going, but maybe seeing that will help.
Um, so as I said before, this is doing kind of like a a balance between the semantic similarity and sort of a a hard overlap of skills. And then you can use that to kind of weight, you know, how you want to find similar people inside of the graph. Um, so you can you can start balancing both sort of the vector search similarity and the uh similarity that uh just happens with inside of the hard matches. And another cool thing about a graph database specifically is I'll go ahead and take this query here.
Um, just so you can see what this looks like.
Oh, go ahead.
Yeah, I mean you could go either way, right? So some of a lot of this to be honest will come down to cost considerations like how expensive is it in Neo forj versus how expensive is it inside of Postgress and that varies a lot depending on the type of infrastructure you have. Having everything in one place means that you it's a little bit you don't have to like sync your data right and then you also the query latency is at least in theory going to be lower because you're just querying from the same database. Um but if you already have data in Postgress maybe or you already have a specialized vector database you also don't have to migrate your data necessarily to Neo forj to make that work. So yeah I'd say a lot of it actually it's performance but it's really like cost per performance right is kind of what you're thinking about in terms of what what does each deployment cost.
Um so this query here um is actually I'm taking I'm looking for similarity between two people and then you see I have this like star dot dot thing here and basically with a graph you're allowed to do what's called variable length queries. So I'm saying hey go out on similar semantic but you're allowed to go out anywhere from zero to two hops between these skill sets before you find a connection between John and Matthew um here. And then I can also union it against just the plain you know person knows same skill. Um and when we get that back we'll see right like you get Matthew over here Matthew knows React John knows HTML and then those are sort of similar because they both have a semantic similarity to JavaScript. Same thing here you see we have this uh semantic similarity but this is only one hop. This is where the variable hop comes in. So you can start to control like these, you know, sort of how far out you can go on on either of these paths to be able to pull back similarities between people.
Um, this is just an advantage of a graph database.
Then I think might want to finish it off for this notebook. Um, I would take a break except that uh we only have 23 more minutes.
So what do you say? Should we just power through the last 20 minutes? You think? Yeah, let's let's let's do that. So now I will we looked at some of the advantages of using the graph and the semantic similarity inside of the graph.
And now we'll talk a little bit about our second module here. And I won't go over to the slides because I think for you guys I can probably just hop right into the notebook around well what if we have just résumés, right? We don't have a CSV file. So this is going to be a simple example that will show you um basically how to take the data um from text and turn it into useful data for the graph. So again like if you're if you're going through and running this live you're just connecting to your same workshop file which you should have from before testing the connection making sure you can count now you should actually get 154 nodes. So here, and if you come by our booth, we can show you much more uh sort of exciting examples than than the two text uh blobs that we have here. Um but here we have two different bios. Um and basically the way that you can do this, and if you've already done some entity extraction, you're probably already familiar with this workflow, is we can define our domain model um in terms of uh pyantic classes. Uh so here basically I'm going to define my person with a name and an email and then a list of skills. Um and then I have the skills field here. Um if you add relationship properties you would have like a nose maybe like in a more complicated model. nose as a relationship would also have like a proficiency property in which case this would be a list of you know nose skills and then you would have a nose skill would have a class um would have a skill property inside of it but you can see this is a very simple example. So all we're doing here is we're defining a list of skills that someone can have. skills just has the name property and then we can create this person list and then once we have our uh pideantic uh class defined uh we can create our system message uh to basically be a prompt for our model um and then we can use in this case I used uh 41 here um and we gave it the documents to uh to ingest and then it will spit out at the end of this some JSON uh with those two people. So, we had two documents. Each one um corresponded to one person. And then we got our emails and skills um with all their different names and such. Um and once we have that right, um it's it's from there it's pretty trivial to load it in. It's very similar to what we did last time. Um in fact if we go down to our graph creation here we'll see uh this isn't exactly the query that we had but uh it's very similar where we've basically we're ingesting one person at a time merging on that uh email address which we have indexed sending the name and then for each of the skills that's sort of a list inside of there um we're going to go ahead and merge uh the skill name and then that nose property connecting them together.
Um, and then of course I could go back to the graph and I can say um, these are Neo Forj employees that I loaded. Um, but I can go ahead and look for one of them in the graph and I should get them back here. Um, where I have them and I have the different skills uh, that they went ahead and uh, picked up um, that I can put inside of the database. So um, very very simple. Uh we have um our own um graph rag Python package as well which is very good for and and also our knowledge graph builder. If you look at some of the code that we used um to implement that which is kind of like a reference UI um which has more sort of examples around if you wanted to do things like document chunking with overlap um you know and and of course also doing like multi-threading with async and stuff like that. So you're not just, you know, doing a for loop, you know, over over a bunch of over a bunch of BIOS. So we have all of that. If you stop by the booth, we'll we can we can give you more around that. Um but like I said, especially for this crowd because you guys are already familiar with this.
Um you know, this is this is a very short module and yes, so we just gone through two, added them to the graph. We now need to go our data.
So So we would have to do that. Yeah.
So, ideally I would have done this in in an order where I would have done this first and then we've we've got we would have gone through like the clustering materializing new relationship with We're we're putting new relationships in these don't have weights on them but if we wanted to reformulate our communities um you go then everyone can hear.
Thank you. So so the the communities I'm kind of following what you're doing. you're adding extra links in between the nodes that have weights on them. Whether it's for semantic distance for skills or whether it's for you know you you are X hops away from somebody else in terms of some other distance computation that you've materialized.
Yeah. So we would we would ideally we run that in a recurring way as we upload data. Right. So because I created I did in this case create I think some new skills in addition to the people. So like has same has similar skill set that we would we would redo that and then we would get a couple more relationships there. Um the semantic one if we created new skills we would create new semantic relationships between the skills. Yes.
Okay. Thank you.
All right. Any other questions before we move on? I'm ready.
All right.
Well, then that was a uh a very quick module. So, I'll go over and cover um just some other topics around this very very quickly um inside of my inside of my slides here.
Um, so what we saw was an example where I'd call it entity extraction or named entity recognition where we were taking a document and we were literally breaking out people, places, and things and relationships from within that document. Um, there's other things that we can do like for example, if we have certain types of documents like from a catalog or in this case RFPs, um, we can start to break things out by actual document structure. So I'm only going to walk through this just so you understand that there's different types of extraction that we can do to create graphs. Um for example, if you know what the anatomy of a document is, like in this case, if we have an RFP, um we know that this RFP is going to be designed in a way where there'll be different sections that we have an intro, objective, proposal, and subsections within that. We can actually create a graph out of those things too. So this is another way that we can do um I would call it more like document extraction where we're actually putting the the metadata of the document and modeling it as a graph. Uh and the advantage of doing things this way is that basically as you start to embed these different pieces um and put them into a knowledge graph.
Um you can basically do these patterns where you can do these searches on either entities that come from different chunks. Um and you can sort of go up and down uh these uh document hierarchies to find things which can be very helpful if you have documents that always have repeated structure. Um so you know that entity sometime connects between those the structures of those documents. You can start to incorporate that inside of your um graph retrieval queries. Um, and then it also gives you a way to do community summaries because we saw Leiden before. Um, but also if you have documents that give you a natural uh hierarchy, um, you have a way of also summarizing information um, across those documents as well.
Yeah. Why do you have entities and documents and chunks in the sameology as opposed to extracting entities and just separate separate documents? Why do you combine the two? Well, I think when they're combined, you you can just do traversals between them. So, what do you what's traversal you want to do? an example that you want to do between a traversal of entities and trunks. Like I say like legal contracts is a good example where if you know like you want to search for different legal clauses but then like the expiry date that might be somewhere else. So like that would be one example. Yeah.
So just making sure I understand.
So like just want to make sure I'm uh clear on what you're talking about traversing the document. So in a legal document say like a data protection clause across multiple vendors or something like that and comparing the language like is that a use case? Yeah. Okay.
Yeah. So that and then like you know so there might be like um like a perpetuity piece or like you know dates and different things and then being able to kind of traverse over that document to find that in addition to the entities.
Anything else? All righty. So, I just wanted to introduce that as as another example of how to do things um for the third module because we're already at 1007. So, let me just go ahead and jump into the thing. So, you'll get to see it.
We'll go over to module three. And this is going to be very simple. So, has has that who here we probably asked this in the beginning. I think we already asked how many people have experience building agents. This is going to be very simple.
It's just going to be a langraph agent that we're going to that we're going to make here. Um, basically what we're going to do is again a similar setup with our environments file. We're going to connect to Neo Forj, test our connection.
Um, there's going to be four tools that we want to build. We want to be able to retrieve the skills of a person. We want to be able to retrieve similar skills to other skills, similar people, like if we wanted to find out who's another good person to work on a thing. Um, and then retrieve people based on a set of skills.
And in this example, um, we're basically going to, um, do a lot of tools first. So, at the end of this notebook, there's going to be like that text to cipher stuff where you get the schema back. But here, what we're going to go over first is actually going to be putting these different tools together. Um, and we do that by graph patterns. And it's the same graph patterns that we've been going over. Uh, so for example here, right, if we just want to find um the skills that someone knows, it's very simple, right? Just person matching to their skills. Um, and as you go down this notebook, basically what you're seeing is all the different patterns. So the second is retrieving people with similar skills. And here we're actually going to use the um the vector index and that similar semantic relationship. Um so we're basically going to pull um actually this one is searching for people with skills. I apologize. So in this one the this is um you're going to look for skills. So for example you if a user puts in different skills those might not match the skills we have inside of the database exactly word for word. So you're going to use vector search to pull out um the specific skills and what's semantically similar to those skills and we can do some scoring thresholds in here to pull back exactly what we want. Um and then that will go ahead and return some skills. So if we had for example right like this continuous delivery cloud native and security um this would be like the types of skills that we pull back from that. the person's similarity. Um, there's a few different ways that we can do that and we've talked about that a lot towards the beginning. Um, we can do it by community. So, we can look for people that know different skills. We can get all of their names and then we can look for um that lighten community that we created. We can look for all the skills that those people know. And basically what we're doing at that point is we're looking for people inside of the same uh skills uh community.
Um but the other way that you can do that is um you can look for similar skill sets um using the uh similar skill set relationship. So the hardcoded relationship that we've made from before uh which basically looks at hey how many how what's the actual skill overlap if you just looked at um who knows what inside of the graph. Um and that will bring back um some answers here between so we were looking at John Garcia and we're saying hey find similar people and then we can get like a score count of overlap um to the to the different people here. Um and then we can start adding in that semantic similarity. So this is where we get this big query right but what this query is actually doing is it's sort of balancing between the um similar skill set and semantically similar skill set. So, it's kind of taking both those scores and adding them together. Um, and then from there, we get uh a floating number score um and a little bit of a different answer that's not just based on hard skill connections, but also skills that are kind of close together. Um, and we can weight those independently as well.
Um, and we can also recommend people given a set of skills. So if we have a set of skills here um we can just do a vector search on um on those skills. Um and then actually this one combination of person skills skills.
Yeah, here it is. Um so basically um the query was broken out into two parts just because this is this is kind of a big thing to look at but the idea with this is we can get um we can basically do vector search on skills get semantically similar skills um and then find person who knows those skills.
So very similar to some of the last ones um and then we can get a skill count for all of those groups and get people back.
Um when we actually define the functions for our agent um we're going to create here um a skills object which basically is just going to help us um with some of our function arguments and returns u but basically first tool retrieve skills of person very simple query um and then we'll have uh down here for tool for tool two when we say find similar skills um what we're going to look at here is again that query where we're going to do that semantic similarity between skills. So we're going to do a vector search to find skills and then we're going to go out one hop on semantic similarity. Um and then we're basically going to collect everything and return it.
And then for the third one we're going to do that waiting um because this tool three is going to be for person similarity. We're going to do that waiting between the similar semantic and the similar skill set with that larger query.
And then I know I'm going through this kind of quickly. For the fourth one where we say find person based on skills here again our entry point is going to be a vector search on skills. Um going out to match those semantically similar skills but then kind of at the end of that uh we'll add on a traversal that will attach the person to knowing those skills count who knows the most and then effectively return that. So those are the four tools that we're going to end up using for this agent. Um when we set up the agent here, if you're familiar with uh how Langraph works, basically uh we we get our LLM, we test that it's alive, we define our list of tools. We can if we didn't want to do this in an agentic way, right? We can just bind our tools to our LLM. Um and we can we can invoke our LLM with tools.
Um, but what we're going to do instead, this is just showing invoking the different the different tools, um, is we are going to go and run it with an agent. So, we're going to use create react agent, which comes from Langraph.
Um, it's one of their pre-built agent, um, that uses the the React um, I don't know if you'd call it a framework, but um, sort of that methodology uh, to build an agent.
And effectively um once we do that, we give it the LLM, we give it the four tools, uh that we had and then um we can see that here we're just testing. We're saying hi and we're just making sure we get some response back. There's a utility function here just to make it easier running in the notebook um which will basically just you know do this some of the um some of the streaming methodology. So, I can just say, "Hey, what skills um does Kristoff have?" And then if I run that, and I don't know if I need to rerun my agent here. Looks like not. Everything's running. Um, so you'll see it says, uh, when I ran that, and actually I ran it for the wrong question here. What skills does Kristoff have? Person named Kristoff. And then it will bring back his skills. So you see there it will choose to use the uh retrieve skills of person. Um and similarly if I went down and I said you know what skills are similar to PowerBI and data visualization um it'll go through and choose the appropriate um you know the the appropriate tool for the job. So in this case, uh, find similar skills. It'll pull those back and you'll see going down, right? If I said, well, what person has similar skills to, you know, this other person here, um, then it will know, oh, I need person similarity. So we'll go ahead and use that specific tool. So in this case, what we're doing is we're providing a bunch of tools that are presumably expert tools that we can give to the model and then it will know that okay, I have to go ahead and pull those um those specific tools to be able to uh provide a response.
And then there's a little app down here as well if you wanted to run the uh chatbot. So um it's a little grad app here.
Um but basically if I ran that I can go ahead and come in here and then I can have a little conversation with it. So this is very small but what skills are similar you know I can go ahead and ask it in here and then provided everything's working it'll go ahead and choose the appropriate tool and then I can say well who knows um you know um maybe I'll just say those skills and it should um go ahead and pull the appropriate tool to be able to find out who knows all these different skills, right? And if I go back to my uh to my example here, I should see the query logic that it used. So, first, you know, it said find similar skills to what I just mentioned because I asked about PowerBI.
And then after that, I asked about uh people who know skills. So, it said find persons based on similar skills. And likewise I can say you know who is um similar to those people in the graph and it will likewise go through and it should you know understand that it needs to use the find other similar persons tools um to be able to do that.
So you'll see if I if I was to keep going down I should get calls um here define persons with similar we have it here.
Yeah person similarity. So it just called the person similarity for each person.
Um and I know we only have a few minutes left. Uh there is if you wanted to run this further basically um I have a text to cipher example. So this is where I'll have an example of passing it the annotated schema. So this is getting to kind of what you were asking about, right? Um where I've provided these descriptions. Um so it's sort of like annotations for the schema as well. And then I can go ahead and give that to an aggregation query function that will have there's also an LLM inside of here that will create the cipher. Um but you can see in here I asked it some questions like describe communities. it was able to understand that it needed to grab, you know, the the match person knows skill and then it needed to grab the lighting community. So, it it knew from the schema, right, that it needed to generate this cipher. Um, and there's a couple more examples of that in the notebook. Um, are there any question? I know I just went over a lot. Um, are there any questions from that that are worth answering now while we have just a couple minutes left? I didn't even know you were here.
How long will the uh Jupyter server be up if we want to play with this? Jupiter server is going to go down very quickly, but if you look at the deck at the end of the deck, I have a link to the code and the data is all in GitHub too. So basically if if you go here, that's the GitHub repository. So you can play with it.
I get the deck.
What's that? The the deck is in this. Do you have access to the Slack channel? So, the deck is in the Slack channel. Um, and I I'll go ahead and and jump to that in a second, but there's the GitHub repository. You can use Aura um console. We have a free trial that you can use. You can just set up a cloud database and you can load the data into there. Um, also before you guys leave, there's a meetup happening tomorrow. Um, it's tonight.
Oh, sorry. tonight at five. Um that and there's a link there for more information on that. Um and then we also have another workshop at uh one o'clock where today was very simple like we're going to go over more graph analytics type of stuff in that workshop. So like the community stuff that I was doing, we're going to dive more into depth on that in that workshop. Um other than that, uh come by our booth if you have more questions. We're going to be wherever right there is. I don't think we have a big expo hall, but if you want to see Neo forj MCP servers, ADK examples, more knowledge graph construction, um all a great uh you know place to come to ask all those types of questions.